{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 82695,
          "databundleVersionId": 9738540,
          "sourceType": "competition"
        },
        {
          "sourceId": 196660105,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 205175997,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 122612,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 103186,
          "modelId": 127417
        },
        {
          "sourceId": 124335,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 104643,
          "modelId": 128845
        }
      ],
      "dockerImageVersionId": 30776,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "[Eedi] Few Shot w/ Qwen2.5-32B-Instruct-AWQ & vLLM",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "pq-1AY7xdyga"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "eedi_mining_misconceptions_in_mathematics_path = kagglehub.competition_download('eedi-mining-misconceptions-in-mathematics')\n",
        "abdullahmeda_qwen2_5_transformers_32b_instruct_awq_1_path = kagglehub.model_download('abdullahmeda/qwen2.5/Transformers/32b-instruct-awq/1')\n",
        "jonathanchan_baai_transformers_bge_large_en_v1_5_1_path = kagglehub.model_download('jonathanchan/baai/Transformers/bge-large-en-v1.5/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "kmTbvw2Xdygq"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "- Load AWQ Quantized Qwen2.5-32B-Instruct using vLLM on 2x T4\n",
        "- For each test question, we retrive a set of similar questions that \"support\" it using train data.\n",
        "    - Consider things like \"construct\" and \"subject\"\n",
        "- Using the set of similar questions, we create a conversation that can be fed to our model\n",
        "- We can then pass these messages into our quantized Qwen2.5-32B-Instruct. Towards the end of the conversation, we give it the question/answer pair we want to predict the misconception for.\n",
        "    - Use the recently supported `chat()` functionality on the conversation\n",
        "    - Generate `n` responses with a slightly higher temperature to create diverse outputs\n",
        "- For each question/answer pair, we now have `n` inferred misconceptions, for each, we retrieve the top 25 misconceptions using BGE embeddings.\n",
        "- The 25 nearest misconceptions for each of our `n` inferred misconceptions for each question/answer pair can now be combined using Borda Ranking.\n",
        "\n"
      ],
      "metadata": {
        "id": "li8jgkbYdygt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Ct93jBD2dygx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import ctypes\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from random import sample\n",
        "from tqdm.auto import tqdm\n",
        "from eedi_metrics import mapk, apk\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Import and set up multiprocessing to avoid fork warnings\n",
        "import multiprocessing\n",
        "multiprocessing.set_start_method('spawn', force=True)\n",
        "\n",
        "# Import ipywidgets for rich notebook output\n",
        "import ipywidgets as widgets\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-04T12:46:05.190742Z",
          "iopub.execute_input": "2024-11-04T12:46:05.1912Z",
          "iopub.status.idle": "2024-11-04T12:46:05.198965Z",
          "shell.execute_reply.started": "2024-11-04T12:46:05.19116Z",
          "shell.execute_reply": "2024-11-04T12:46:05.197496Z"
        },
        "id": "iT8hGpl7dygy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "8KNXRwPCdyg0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]   = \"0,1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def clean_memory(deep=False):\n",
        "    gc.collect()\n",
        "    if deep:\n",
        "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "_uuid": "48bd86db-c8df-454c-b0e5-275b9f019a10",
        "_cell_guid": "029b170b-c7d8-401c-aed2-ae41f5385f69",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-11-04T12:46:12.071188Z",
          "iopub.execute_input": "2024-11-04T12:46:12.071627Z",
          "iopub.status.idle": "2024-11-04T12:46:12.07999Z",
          "shell.execute_reply.started": "2024-11-04T12:46:12.071588Z",
          "shell.execute_reply": "2024-11-04T12:46:12.079032Z"
        },
        "trusted": true,
        "id": "bcNnCAM3dyg0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "idtS9BzRdyg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "\n",
        "train_eval = True\n",
        "n_train_eval_rows = 100\n",
        "\n",
        "comp_dir  = '/kaggle/input/eedi-mining-misconceptions-in-mathematics'\n",
        "\n",
        "llm_model_pth   = '/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1'\n",
        "embed_model_pth = '/kaggle/input/baai/transformers/bge-large-en-v1.5/1'\n",
        "\n",
        "\n",
        "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
        "    train_eval = False"
      ],
      "metadata": {
        "_uuid": "7e9d6c3d-5dbe-4e17-90af-597deb8cac96",
        "_cell_guid": "1875b764-6f35-4749-b9ad-04549659b4fb",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-11-04T12:46:17.310177Z",
          "iopub.execute_input": "2024-11-04T12:46:17.311118Z",
          "iopub.status.idle": "2024-11-04T12:46:17.316346Z",
          "shell.execute_reply.started": "2024-11-04T12:46:17.31106Z",
          "shell.execute_reply": "2024-11-04T12:46:17.315301Z"
        },
        "trusted": true,
        "id": "xJ0c6Pt2dyg4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if train_eval:\n",
        "    test       = pd.read_csv(f'{comp_dir}/train.csv').sample(n_train_eval_rows, random_state=3)\n",
        "    test       = test.sort_values(['QuestionId'], ascending=True).reset_index(drop=True)\n",
        "else:\n",
        "    test       = pd.read_csv(f'{comp_dir}/test.csv')\n",
        "\n",
        "train          = pd.read_csv(f'{comp_dir}/train.csv')\n",
        "sample_sub     = pd.read_csv(f'{comp_dir}/sample_submission.csv')\n",
        "misconceptions = pd.read_csv(f'{comp_dir}/misconception_mapping.csv')\n",
        "\n",
        "len(train), len(test), len(misconceptions)"
      ],
      "metadata": {
        "_uuid": "a0a602f7-7a5f-4fb9-b783-97fc47802cda",
        "_cell_guid": "0c03a53e-28bf-42b2-909b-7754e0a95cc7",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-11-04T12:46:23.729963Z",
          "iopub.execute_input": "2024-11-04T12:46:23.730453Z",
          "iopub.status.idle": "2024-11-04T12:46:23.807362Z",
          "shell.execute_reply.started": "2024-11-04T12:46:23.73041Z",
          "shell.execute_reply": "2024-11-04T12:46:23.806162Z"
        },
        "trusted": true,
        "id": "WGR2auUqdyg5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spin up our `Qwen2.5-32B-Instruct-AWQ` using vLLM\n",
        "\n",
        "Credits: https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm\n",
        "\n",
        "The model fits very comfortably even with a max model length of 4096. Even longer can could be experimented with. Incase of OOM errors, it might be helpful to decrease `max_num_seqs` to 4 or 8, or even 1! (Default is 256)"
      ],
      "metadata": {
        "id": "2uc0mGe2dyg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(\n",
        "    llm_model_pth,\n",
        "    trust_remote_code=True,\n",
        "    dtype=\"half\", max_model_len=4096,\n",
        "    tensor_parallel_size=2, gpu_memory_utilization=0.95,\n",
        ")\n",
        "\n",
        "tokenizer = llm.get_tokenizer()"
      ],
      "metadata": {
        "_uuid": "d1945dd4-967b-4df0-af92-dcc6e7a7b8e4",
        "_cell_guid": "f4967dbe-eb0d-425c-8c46-ccc3539b9175",
        "_kg_hide-output": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-04T13:12:24.740755Z",
          "iopub.execute_input": "2024-11-04T13:12:24.741556Z",
          "iopub.status.idle": "2024-11-04T13:22:56.374286Z",
          "shell.execute_reply.started": "2024-11-04T13:12:24.74151Z",
          "shell.execute_reply": "2024-11-04T13:22:56.372793Z"
        },
        "id": "-x5tXLIMdyg8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post process data"
      ],
      "metadata": {
        "id": "MIsAemnedyg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_cols         = [\"AnswerAText\", \"AnswerBText\", \"AnswerCText\", \"AnswerDText\"]\n",
        "misconception_cols  = [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"]\n",
        "\n",
        "keep_cols           = [\"QuestionId\", \"CorrectAnswer\", \"ConstructName\", \"SubjectName\", \"QuestionText\" ]\n",
        "\n",
        "def wide_to_long(df: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    # Melt the answer columns\n",
        "    answers_df = pd.melt(\n",
        "        id_vars=keep_cols,\n",
        "        frame=df[keep_cols + answer_cols],\n",
        "        var_name='Answer', value_name='Value'\n",
        "    ).sort_values([\"QuestionId\", \"Answer\"]).reset_index(drop=True)\n",
        "\n",
        "    if misconception_cols[0] not in df.columns:  # If test set\n",
        "        return answers_df\n",
        "\n",
        "    # Melt the misconception columns\n",
        "    misconceptions_df = pd.melt(\n",
        "        id_vars=keep_cols,\n",
        "        frame=df[keep_cols + misconception_cols],\n",
        "        var_name='Misconception', value_name='MisconceptionId'\n",
        "    ).sort_values([\"QuestionId\", \"Misconception\"]).reset_index(drop=True)\n",
        "\n",
        "    answers_df[['Misconception', 'MisconceptionId']] = misconceptions_df[['Misconception', 'MisconceptionId']]\n",
        "\n",
        "    return answers_df\n",
        "\n",
        "\n",
        "test  = wide_to_long(test)\n",
        "train = wide_to_long(train)\n",
        "\n",
        "test['AnswerId']  = test.Answer.str.replace('Answer', '').str.replace('Text', '')\n",
        "train['AnswerId'] = train.Answer.str.replace('Answer', '').str.replace('Text', '')\n",
        "\n",
        "train = pd.merge(train, misconceptions, on='MisconceptionId', how='left')\n",
        "if train_eval:\n",
        "    test = pd.merge(test, misconceptions, on='MisconceptionId', how='left')"
      ],
      "metadata": {
        "_uuid": "75ee62af-8be4-44bf-932e-f9c7350345ae",
        "_cell_guid": "a7609fc8-f752-46d5-b866-3c14b62323d3",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-04T13:25:50.313121Z",
          "iopub.execute_input": "2024-11-04T13:25:50.314034Z",
          "iopub.status.idle": "2024-11-04T13:25:50.479794Z",
          "shell.execute_reply.started": "2024-11-04T13:25:50.313989Z",
          "shell.execute_reply": "2024-11-04T13:25:50.478071Z"
        },
        "id": "IMAABBMEdyg9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(3)"
      ],
      "metadata": {
        "_uuid": "80fb66c2-28fe-4826-8050-736356e5c298",
        "_cell_guid": "45fe473b-f4ad-4425-b940-fd5fadb65e9c",
        "execution": {
          "iopub.status.busy": "2024-11-04T13:25:39.417876Z",
          "iopub.execute_input": "2024-11-04T13:25:39.418323Z",
          "iopub.status.idle": "2024-11-04T13:25:39.464326Z",
          "shell.execute_reply.started": "2024-11-04T13:25:39.418281Z",
          "shell.execute_reply": "2024-11-04T13:25:39.463408Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "y7PKj-vcdyg-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(3)"
      ],
      "metadata": {
        "_uuid": "b0c18856-3a74-4c00-b46a-0280aeb3bf0b",
        "_cell_guid": "54bea772-f8ca-4104-b527-38176e40a476",
        "execution": {
          "iopub.status.busy": "2024-11-04T13:26:21.04112Z",
          "iopub.execute_input": "2024-11-04T13:26:21.042055Z",
          "iopub.status.idle": "2024-11-04T13:26:21.058868Z",
          "shell.execute_reply.started": "2024-11-04T13:26:21.042012Z",
          "shell.execute_reply": "2024-11-04T13:26:21.057669Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "9DiVo-9pdyg_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions\n",
        "\n",
        "Credits: https://www.kaggle.com/code/richolson/eedi-phi-mini-3-5"
      ],
      "metadata": {
        "id": "vM_E0G0odyg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the most similar question_ids' given the subject and construct\n",
        "\n",
        "The following function return `top_k` number of question ids' first by checking for questions that have both construct and subject to be similar.\n",
        "\n",
        "If that does not come up to `top_k`, questions with either similar subject or construct are chosen. If we are still short of question ids', we select random questions for the remainder of `top_k`"
      ],
      "metadata": {
        "id": "6s2wpvz1dyhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topk_similar_rows(question_id: int, construct: str, subject: str, top_k: int) -> list[int]:\n",
        "    \"\"\" Gets the top n ids of questions that most similar to the given construct and subject \"\"\"\n",
        "\n",
        "    # Rows with similar construct and subject\n",
        "    similar_cs_rows = train[(train.ConstructName == construct) & (train.SubjectName == subject)]\n",
        "    similar_cs_qids = list(set(similar_cs_rows.QuestionId.values.tolist()))\n",
        "\n",
        "    if train_eval and question_id in similar_cs_qids:\n",
        "        similar_cs_qids.remove(question_id)\n",
        "\n",
        "    if len(similar_cs_qids) >= top_k:\n",
        "        k_similar_cs_qids = sample(similar_cs_qids, top_k)\n",
        "        return k_similar_cs_qids\n",
        "\n",
        "\n",
        "    # Rows with similar construct or subject for remainder of top_k\n",
        "    similar_s_rows = train[(train.ConstructName != construct) & (train.SubjectName == subject)]\n",
        "    similar_c_rows = train[(train.ConstructName == construct) & (train.SubjectName != subject)]\n",
        "    similar_c_or_s_qids = list(set(similar_s_rows.QuestionId.values.tolist() + similar_c_rows.QuestionId.values.tolist()))\n",
        "\n",
        "    if train_eval and question_id in similar_c_or_s_qids:\n",
        "        similar_c_or_s_qids.remove(question_id)\n",
        "\n",
        "    if len(similar_c_or_s_qids) >= top_k - len(similar_cs_qids):\n",
        "        n_similar_c_or_s_qids = sample(similar_c_or_s_qids, top_k - len(similar_cs_qids))\n",
        "        return similar_cs_qids + n_similar_c_or_s_qids\n",
        "\n",
        "\n",
        "    # Random rows for remainder of top_k\n",
        "    not_so_similar_rows = train[(train.ConstructName != construct) & (train.SubjectName != subject)]\n",
        "    not_so_similar_rows_qids = list(set(not_so_similar_rows.QuestionId.values.tolist()))\n",
        "\n",
        "    if train_eval and question_id in not_so_similar_rows_qids:\n",
        "        not_so_similar_rows_qids.remove(question_id)\n",
        "\n",
        "    n_not_so_similar_rows_qids = sample(not_so_similar_rows_qids, top_k - len(similar_c_or_s_qids))\n",
        "    return similar_c_or_s_qids + n_not_so_similar_rows_qids"
      ],
      "metadata": {
        "_uuid": "295d3bad-3a81-4ea2-8dd6-da8716f8aebd",
        "_cell_guid": "3d201b2d-1b92-449f-b1a0-60e710474907",
        "execution": {
          "iopub.status.busy": "2024-11-04T13:26:25.719945Z",
          "iopub.execute_input": "2024-11-04T13:26:25.720885Z",
          "iopub.status.idle": "2024-11-04T13:26:25.73296Z",
          "shell.execute_reply.started": "2024-11-04T13:26:25.720837Z",
          "shell.execute_reply": "2024-11-04T13:26:25.731578Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "2JVkAj40dyhA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the chat conversation for each question"
      ],
      "metadata": {
        "id": "-IashTpPdyhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_conversation_msgs(question, correct_ans, incorrect_ans, misconception):\n",
        "    msgs = [\n",
        "        {'role': 'user',      'content': 'Question: ' + question.strip()},\n",
        "        {'role': 'assistant', 'content': 'Provide me with the correct answer for a baseline.'},\n",
        "        {'role': 'user',      'content': 'Correct Answer: ' + correct_ans.strip()},\n",
        "        {'role': 'assistant', 'content': 'Now provide the incorrect answer and I will anaylze the difference to infer the misconception.'},\n",
        "        {'role': 'user',      'content': 'Incorrect Answer: ' + incorrect_ans.strip()},\n",
        "    ]\n",
        "\n",
        "    if misconception is not None:\n",
        "        msgs += [{'role': 'assistant', 'content': 'Misconception for incorrect answer: ' + misconception}]\n",
        "\n",
        "    return msgs"
      ],
      "metadata": {
        "_uuid": "2d0cb0cb-40e7-4345-8978-a71806171992",
        "_cell_guid": "586cb3ea-6da3-406a-b159-2864b9314790",
        "execution": {
          "iopub.status.busy": "2024-11-04T13:26:31.393939Z",
          "iopub.execute_input": "2024-11-04T13:26:31.394931Z",
          "iopub.status.idle": "2024-11-04T13:26:31.401375Z",
          "shell.execute_reply.started": "2024-11-04T13:26:31.394885Z",
          "shell.execute_reply": "2024-11-04T13:26:31.400357Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "veTh6-lydyhB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infer the misconception using `llm.chat()`\n",
        "\n",
        "Note: `llm.chat()` has only been introduced recently and is available only in the later releases\n",
        "\n",
        "We generate n outputs, with a higher temperature to create diverse representations of the outputs, which can then be used later to rank our results!"
      ],
      "metadata": {
        "id": "RtkCXD62dyhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_params = SamplingParams(\n",
        "    n=10,                     # Number of output sequences to return for each prompt.\n",
        "    # top_p=0.5,               # Float that controls the cumulative probability of the top tokens to consider.\n",
        "    temperature=0.7,          # randomness of the sampling\n",
        "    seed=1,                   # Seed for reprodicibility\n",
        "    skip_special_tokens=True, # Whether to skip special tokens in the output.\n",
        "    max_tokens=64,            # Maximum number of tokens to generate per output sequence.\n",
        "    stop=['\\n\\n', '. '],      # List of strings that stop the generation when they are generated.\n",
        ")"
      ],
      "metadata": {
        "_uuid": "ca7a0849-8032-4fe2-b95b-3ea5775e1d99",
        "_cell_guid": "8728080d-4c3a-472d-9b3c-f410021bba1e",
        "execution": {
          "iopub.status.busy": "2024-11-04T13:26:35.75189Z",
          "iopub.execute_input": "2024-11-04T13:26:35.752902Z",
          "iopub.status.idle": "2024-11-04T13:26:35.758591Z",
          "shell.execute_reply.started": "2024-11-04T13:26:35.752854Z",
          "shell.execute_reply": "2024-11-04T13:26:35.757433Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "yojS38dpdyhC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission = []\n",
        "for idx, row in tqdm(test.iterrows(), total=len(test)):\n",
        "\n",
        "    if idx % 50:\n",
        "        clean_memory()\n",
        "        clean_memory()\n",
        "\n",
        "    if row['CorrectAnswer'] == row['AnswerId']: continue\n",
        "    if train_eval and not row['MisconceptionId'] >= 0: continue\n",
        "\n",
        "    context_qids   = get_topk_similar_rows(row['QuestionId'], row['ConstructName'], row['SubjectName'], k)\n",
        "    correct_answer = test[(test.QuestionId == row['QuestionId']) & (test.CorrectAnswer == test.AnswerId)].Value.tolist()[0]\n",
        "\n",
        "    messages = []\n",
        "    for qid in context_qids:\n",
        "        correct_option = train[(train.QuestionId == qid) & (train.CorrectAnswer == train.AnswerId)]\n",
        "        incorrect_options = train[(train.QuestionId == qid) & (train.CorrectAnswer != train.AnswerId)]\n",
        "\n",
        "        for idx, incorrect_option in incorrect_options.iterrows():\n",
        "            if type(incorrect_option['MisconceptionName']) == str: # Filter out NaNs\n",
        "                messages += get_conversation_msgs(\n",
        "                    question = correct_option.QuestionText.tolist()[0],\n",
        "                    correct_ans = correct_option.Value.tolist()[0],\n",
        "                    incorrect_ans = incorrect_option['Value'],\n",
        "                    misconception = incorrect_option['MisconceptionName'],\n",
        "                )\n",
        "\n",
        "    # Coversation for Incorrect answer to get misconception for\n",
        "    messages += get_conversation_msgs(\n",
        "        question = row['QuestionText'],\n",
        "        correct_ans = correct_answer,\n",
        "        incorrect_ans = row['Value'],\n",
        "        misconception = None,\n",
        "    )\n",
        "\n",
        "    output = llm.chat(messages, sampling_params, use_tqdm=False)\n",
        "    inferred_misconceptions = [imc.text.split(':')[-1].strip() for imc in output[0].outputs]\n",
        "\n",
        "    if not train_eval:\n",
        "        submission.append([f\"{row['QuestionId']}_{row['AnswerId']}\", inferred_misconceptions])\n",
        "    else:\n",
        "        submission.append([\n",
        "            f\"{row['QuestionId']}_{row['AnswerId']}\",\n",
        "            inferred_misconceptions,\n",
        "            context_qids,\n",
        "            [int(row['MisconceptionId'])],\n",
        "            row['MisconceptionName']\n",
        "        ])\n",
        "\n",
        "\n",
        "submission = pd.DataFrame(submission, columns=['QuestionId_Answer', 'InferredMisconception', 'TopKQuestionIDs',\n",
        "                                               'MisconceptionIdGT', 'MisconceptionNameGT'][:len(submission[0])])\n",
        "\n",
        "len(submission)"
      ],
      "metadata": {
        "_uuid": "94b7e3dc-d1a5-480b-b1a3-948b6fe05fa9",
        "_cell_guid": "5e971219-1c18-4a99-988a-c1da5a3f95c7",
        "execution": {
          "iopub.status.busy": "2024-11-04T13:26:39.412592Z",
          "iopub.execute_input": "2024-11-04T13:26:39.41346Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "5-NimdFydyhC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission.head()"
      ],
      "metadata": {
        "_uuid": "ac6e657d-c08a-42ef-94b6-32bd4b3ffe8a",
        "_cell_guid": "fe5db7db-7c29-4640-afb4-74407ffaf0d1",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:03.700178Z",
          "iopub.execute_input": "2024-10-05T11:15:03.700602Z",
          "iopub.status.idle": "2024-10-05T11:15:03.7129Z",
          "shell.execute_reply.started": "2024-10-05T11:15:03.700561Z",
          "shell.execute_reply": "2024-10-05T11:15:03.711934Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "KJqh7XKXdyhD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find most similar misconceptions\n",
        "\n",
        "Delete the model and clean memory to load up embedding model"
      ],
      "metadata": {
        "id": "ekjF24KFdyhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del llm\n",
        "\n",
        "clean_memory(deep=True)\n",
        "clean_memory(deep=True)"
      ],
      "metadata": {
        "_uuid": "bb488ea2-44d9-4fd4-8708-ac26b33c730a",
        "_cell_guid": "b026875b-c132-46ad-bd37-714480a3eecc",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:04.653635Z",
          "iopub.execute_input": "2024-10-05T11:15:04.654275Z",
          "iopub.status.idle": "2024-10-05T11:15:05.638062Z",
          "shell.execute_reply.started": "2024-10-05T11:15:04.654234Z",
          "shell.execute_reply": "2024-10-05T11:15:05.637127Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "1tl0D59adyhF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer   = AutoTokenizer.from_pretrained(embed_model_pth, trust_remote_code=True)\n",
        "embed_model = AutoModel.from_pretrained(embed_model_pth, trust_remote_code=True).to(\"cuda:0\")"
      ],
      "metadata": {
        "_uuid": "67f121a8-262f-4d86-ab35-7c7d044b4131",
        "_cell_guid": "f0141914-5a34-4e31-be1a-a48fa39d1710",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:06.811666Z",
          "iopub.execute_input": "2024-10-05T11:15:06.813353Z",
          "iopub.status.idle": "2024-10-05T11:15:23.174912Z",
          "shell.execute_reply.started": "2024-10-05T11:15:06.813177Z",
          "shell.execute_reply": "2024-10-05T11:15:23.174033Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "XgKQzldBdyhF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(texts, batch_size=8):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=1024).to('cuda:0')\n",
        "        with torch.no_grad():\n",
        "            outputs = embed_model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
        "        all_embeddings.append(embeddings.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(all_embeddings, axis=0)"
      ],
      "metadata": {
        "_uuid": "205d3c34-9fcf-4d61-a946-72c9ffd24bf9",
        "_cell_guid": "0b6b47b0-b593-4214-8dd4-abcd4b09389c",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:23.176641Z",
          "iopub.execute_input": "2024-10-05T11:15:23.176974Z",
          "iopub.status.idle": "2024-10-05T11:15:23.184249Z",
          "shell.execute_reply.started": "2024-10-05T11:15:23.17694Z",
          "shell.execute_reply": "2024-10-05T11:15:23.183069Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "LN6XNOurdyhG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "all_ctx_vector  = generate_embeddings(list(misconceptions.MisconceptionName.values))\n",
        "\n",
        "all_ctx_vector.shape"
      ],
      "metadata": {
        "_uuid": "be5bc847-9c5c-4235-a855-f03ea8a1e7cb",
        "_cell_guid": "99f37563-fba1-42e7-927d-b5ce07c4d4f3",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:23.185445Z",
          "iopub.execute_input": "2024-10-05T11:15:23.185928Z",
          "iopub.status.idle": "2024-10-05T11:15:37.938086Z",
          "shell.execute_reply.started": "2024-10-05T11:15:23.185879Z",
          "shell.execute_reply": "2024-10-05T11:15:37.937253Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "IfrkStjHdyhH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "n_results = []\n",
        "\n",
        "for results in tqdm(pd.DataFrame(submission.InferredMisconception.values.tolist()).T.values):\n",
        "    all_text_vector = generate_embeddings(list(results))\n",
        "    cosine_similarities = cosine_similarity(all_text_vector, all_ctx_vector)\n",
        "    test_sorted_indices = np.argsort(-cosine_similarities, axis=1)\n",
        "    n_results.append(test_sorted_indices)\n",
        "\n",
        "n_results = np.array(n_results)\n",
        "n_results.shape"
      ],
      "metadata": {
        "_uuid": "75086bbc-4c8a-4b26-834c-4f507402c699",
        "_cell_guid": "9cbd2e70-d9cb-44e0-9639-2a47349dde42",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:37.940437Z",
          "iopub.execute_input": "2024-10-05T11:15:37.941167Z",
          "iopub.status.idle": "2024-10-05T11:15:38.981268Z",
          "shell.execute_reply.started": "2024-10-05T11:15:37.941112Z",
          "shell.execute_reply": "2024-10-05T11:15:38.980146Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "vjpXz1tZdyhI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "n_results = np.transpose(n_results, (1, 0, 2))\n",
        "n_results.shape"
      ],
      "metadata": {
        "_uuid": "dc8297b3-b1d7-45d0-9b67-2f122991283f",
        "_cell_guid": "f304d958-0a24-4768-afba-4312e51020a5",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:38.982788Z",
          "iopub.execute_input": "2024-10-05T11:15:38.984Z",
          "iopub.status.idle": "2024-10-05T11:15:38.991717Z",
          "shell.execute_reply.started": "2024-10-05T11:15:38.98395Z",
          "shell.execute_reply": "2024-10-05T11:15:38.990563Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "8TZKLQJJdyhJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine ranking of each generated output for each question\n",
        "\n",
        "Borda count is a very simple ranking mechanism"
      ],
      "metadata": {
        "id": "kcqfrUdgdyhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def borda_count(rankings):\n",
        "    scores = {}\n",
        "    num_elements = len(next(iter(rankings)))\n",
        "\n",
        "    for model_ranking in rankings:\n",
        "        for idx, item in enumerate(model_ranking):\n",
        "            points = num_elements - idx\n",
        "            scores[item] = scores.get(item, 0) + points\n",
        "\n",
        "    # Sort the misconceptions based on total points\n",
        "    final_ranking = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ranked_results = [r for r, score in final_ranking]\n",
        "    return ranked_results\n",
        "\n",
        "# Compute the final ranking\n",
        "final_rankings = np.array([borda_count(result) for result in n_results])\n",
        "\n",
        "final_rankings.shape"
      ],
      "metadata": {
        "_uuid": "3b7bf00b-4c34-4ce6-9fce-d598c3a048e0",
        "_cell_guid": "b1d9184a-055b-461e-9d54-6c1ad989d352",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:44.895153Z",
          "iopub.execute_input": "2024-10-05T11:15:44.895929Z",
          "iopub.status.idle": "2024-10-05T11:15:45.06506Z",
          "shell.execute_reply.started": "2024-10-05T11:15:44.895887Z",
          "shell.execute_reply": "2024-10-05T11:15:45.064097Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "OO5w0K7QdyhL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission['MisconceptionId'] = final_rankings[:, :25].tolist()"
      ],
      "metadata": {
        "_uuid": "2877b81b-2b3c-4694-a818-cdd40a111b2d",
        "_cell_guid": "1249b3ad-8295-44a4-b8b9-3c4ec09cd62b",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:45.117098Z",
          "iopub.execute_input": "2024-10-05T11:15:45.117434Z",
          "iopub.status.idle": "2024-10-05T11:15:45.122366Z",
          "shell.execute_reply.started": "2024-10-05T11:15:45.117395Z",
          "shell.execute_reply": "2024-10-05T11:15:45.121375Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "jiGy1m_kdyhM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit :)"
      ],
      "metadata": {
        "id": "NAQh7LwPdyhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_eval:\n",
        "    submission['apk@25'] = submission.apply(lambda row: apk(row['MisconceptionIdGT'], row['MisconceptionId']), axis=1)\n",
        "    submission.to_csv('submission_debug.csv', index=False)\n",
        "\n",
        "    print(submission['apk@25'].mean())"
      ],
      "metadata": {
        "_uuid": "9ca8d18d-9f35-4160-9f79-d727e952f95d",
        "_cell_guid": "93309060-9185-4a2e-8fc7-2ec398934160",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:45.493097Z",
          "iopub.execute_input": "2024-10-05T11:15:45.49344Z",
          "iopub.status.idle": "2024-10-05T11:15:45.498559Z",
          "shell.execute_reply.started": "2024-10-05T11:15:45.493403Z",
          "shell.execute_reply": "2024-10-05T11:15:45.497622Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "sRk4CjeOdyhN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission[\"MisconceptionId\"] = submission[\"MisconceptionId\"].apply(lambda x: ' '.join(map(str, x)))\n",
        "submission[['QuestionId_Answer', 'MisconceptionId']].to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "_uuid": "ea1b9622-0511-4912-bd58-3576a1195a1a",
        "_cell_guid": "deb71ff1-2e1d-472e-be47-f92f4e8c5da6",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:46.078517Z",
          "iopub.execute_input": "2024-10-05T11:15:46.078897Z",
          "iopub.status.idle": "2024-10-05T11:15:46.103085Z",
          "shell.execute_reply.started": "2024-10-05T11:15:46.07886Z",
          "shell.execute_reply": "2024-10-05T11:15:46.102352Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "jZ73Tn7_dyhO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission.head(25)"
      ],
      "metadata": {
        "_uuid": "6ed03390-4a94-43e5-bf53-74a9394ef1c0",
        "_cell_guid": "b0e78b75-b055-4c5d-8b87-be54c4ebcb8f",
        "execution": {
          "iopub.status.busy": "2024-10-05T11:15:46.458192Z",
          "iopub.execute_input": "2024-10-05T11:15:46.459087Z",
          "iopub.status.idle": "2024-10-05T11:15:46.47293Z",
          "shell.execute_reply.started": "2024-10-05T11:15:46.459048Z",
          "shell.execute_reply": "2024-10-05T11:15:46.471969Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "rpWYSGvOdyhO"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}